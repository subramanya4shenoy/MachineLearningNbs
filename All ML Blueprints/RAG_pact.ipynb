{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlY6uT-pq0H7"
      },
      "source": [
        "#RAG setup\n",
        "\n",
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k3cgfVKUeotm",
        "outputId": "24f89763-7ef8-47b7-debe-3635b9ff7dd7"
      },
      "outputs": [],
      "source": [
        "%pip install langchain_community\n",
        "%pip install langchain_experimental\n",
        "%pip install langchain-openai\n",
        "%pip install langchainhub\n",
        "%pip install chromadb\n",
        "%pip install langchain\n",
        "%pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "aEpwbd6Fi4Ug"
      },
      "outputs": [],
      "source": [
        "# importing the packages\n",
        "import os\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4\n",
        "import openai\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "import chromadb\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_experimental.text_splitter import SemanticChunker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QWHM2cQalxcv"
      },
      "outputs": [],
      "source": [
        "## setting up your OpenAi key\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-YOUR_KEY'\n",
        "openai.api_key = os.environ['OPENAI_API_KEY']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "si8JBOKuq63V"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "1.   Web loading and crawling.\n",
        "2.   Splitting the data into digestible chunks for the Chroma DB vectorizing algorithm.\n",
        "3. Embedding and indexing those chunks.\n",
        "4. Adding those chunks and embeddings to the Chroma DB vector store.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HbkKGLKYqJtN"
      },
      "outputs": [],
      "source": [
        "# Web loading and crawling\n",
        "\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://kbourne.github.io/chapter1.html\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "           class_=(\"post-content\", \"post-title\",\n",
        "                   \"post-header\")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "\n",
        "docs = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09jPdIEtUmfr"
      },
      "source": [
        "## Splitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nK10VpZ2Rpv2"
      },
      "outputs": [],
      "source": [
        "text_splitter = SemanticChunker(OpenAIEmbeddings()) # We are not specifying which model to be used for embedding so it will be using text-embedding-ada-002  by default.\n",
        "splits = text_splitter.split_documents(docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFjmqokSkpK2"
      },
      "source": [
        "## Embedding and indexing the chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "1pnNaR26kWn5"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "retriever = vectorstore.as_retriever()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pk8Lg3xqYSe",
        "outputId": "0b5f12a1-ab7a-4a1d-dbc0-6a8b91e5e66a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\\nEstablished Large Language Models (LLM), what we call the foundation models, can learn in two ways:\\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\\'s intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?'),\n",
              " Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\\nEstablished Large Language Models (LLM), what we call the foundation models, can learn in two ways:\\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\\'s intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?'),\n",
              " Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Can you imagine what you could do with all of the benefits mentioned above, but combined with all of the data within your company, about everything your company has ever done, about your customers and all of their interactions, or about all of your products and services combined with a knowledge of what a specific customer’s needs are? You do not have to imagine it, that is what RAG does! Even smaller companies are not able to access much of their internal data resources very effectively. Larger companies are swimming in petabytes of data that is not readily accessible or is not being fully utilized. Prior to RAG, most of the services you saw that connected customers or employees with the data resources of the company were really just scratching the surface of what is possible compared to if they could access ALL of the data in the company. With the advent of RAG and generative AI in general, corporations are on the precipice of something really, really big. Comparing RAG with Model Fine-Tuning#\\nEstablished Large Language Models (LLM), what we call the foundation models, can learn in two ways:\\n Fine-tuning - With fine-tuning, you are adjusting the weights and/or biases that define the model\\'s intelligence based on new training data. This directly impacts the model, permanently changing how it will interact with new inputs. Input/Prompts - This is where you actually \"use\" the model, using the prompt/input to introduce new knowledge that the LLM can act upon. Why not use fine-tuning in all situations?'),\n",
              " Document(metadata={'source': 'https://kbourne.github.io/chapter1.html'}, page_content='Maintaining this integration over time, especially as data sources evolve or expand, adds even more complexity and cost. Organizations need to invest in technical expertise and infrastructure to effectively leverage RAG capabilities while accounting for the rapid increase in complexities these systems bring with them. Potential for Information Overload: \\nIt is very possible for RAG-based systems to pull in too much information. It is just as important to implement mechanisms to address this issue as it is to handle times when not enough relevant information is found. Determining the relevance and importance of retrieved information to be included in the final output requires sophisticated filtering and ranking mechanisms. Without these, the quality of the generated content could be compromised by an excess of unnecessary or marginally relevant details. RAG Vocabulary#\\nNow is as good a time as any to review some vocabulary that should help you get familiar with the various concepts in RAG. This is not an exhaustive list, but understanding these core concepts should help you understand everything else we teach you about RAG in a more effective way:\\nLarge Language Model (LLM)\\nMost of this book will deal with LLMs. LLMs are generative AI technologies that focus on generating text.')]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query = \"Is Rag really worth it?\"\n",
        "relevant_docs = retriever.get_relevant_documents(query)\n",
        "relevant_docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "685J60ipvm31"
      },
      "source": [
        "## Retrieval and generation\n",
        "\n",
        "Take in a user query.\n",
        "\n",
        "*   Vectorize that user query.\n",
        "* Perform a similarity search of the vector store to find the closest vectors to the\n",
        "* user query vector, as well as their associated content.\n",
        "* Pass the retrieved content into a prompt template, a process known as hydrating.\n",
        "* Pass that hydrated prompt to the LLM.\n",
        "* Once you receive a response from the LLM, present it to the user.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGkdfdQ3y1Gu"
      },
      "source": [
        "## Prompt templates from the LangChain Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nbxLyCmc708T",
        "outputId": "6b0abc1d-beef-4c1d-fa07-31d9944e0e7a"
      },
      "outputs": [],
      "source": [
        "%pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "LFPdMk3c77G1"
      },
      "outputs": [],
      "source": [
        "LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "LANGCHAIN_API_KEY=\"lsv2_YOUR_LANGCHAIN_API_KEY\"\n",
        "LANGCHAIN_PROJECT=\"pr-giving-pegboard-42\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f89CzO2s-zu",
        "outputId": "3ca7266b-433e-419c-d69d-3bd89a192fa9"
      },
      "outputs": [],
      "source": [
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiQsaq-U9YRS"
      },
      "source": [
        "## Formatting a function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QVhAJmdNzkKN"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfyijK33-4Ev"
      },
      "source": [
        "## Defining your LLM\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "77lIhR0W-d-N"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1-bw74XAy1i"
      },
      "source": [
        "## Setting up a LangChain chain using LCEL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "7W7h2oyT-95D"
      },
      "outputs": [],
      "source": [
        "rag_chain = ({\"context\": retriever | format_docs, \"question\": RunnablePassthrough()} | prompt | llm | StrOutputParser())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3EovtI_eFun"
      },
      "source": [
        "## Submitting a question for RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "2Xn7vzw6UVw7",
        "outputId": "c5887f19-f6cd-49b5-9810-faba1a203ddd"
      },
      "outputs": [],
      "source": [
        "rag_chain.invoke(\"what is rag?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QW3N77ImeTCf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
