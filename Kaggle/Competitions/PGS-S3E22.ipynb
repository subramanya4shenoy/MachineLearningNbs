{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ⛳️ 1. Look at the Big picture \n🟢 Read, Understand the Problem\n\n### 🔸 Frame the problem\n* 🥇 Note what we want to acheive.\n* 🥇 Form a hypothesis and decide the end goal and identify the format to acheive.\n* 🥇 Decide on :\n* * 🥈 Learning? : Supervised / Unsupervised / Reinfored\n* * 🥈 Problem type? : Classification/Regression\n* * 🥈 Univariate/BiVariate ?\n* * 🥈 Single target or Multi target ?\n\n### 🔸 Select Performance measure\n* 🥇  RMSE\n* 🥇  MAE (best fit if there are many outliers)\n\n### 🔸 Check for Assumptions\n* 🥇 Check for general information about the topic. Get the assumptions and features which needs more conciderations. etc..","metadata":{}},{"cell_type":"markdown","source":"\n〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰\n# Observation\n\n* 🥇 want to get a column which says horse survived or not (lived / died)\n* \n〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰〰\n\n","metadata":{}},{"cell_type":"markdown","source":"# ⛳️ 2. Get the Data\n🟢 Make sure you collect the data from trusted sources and legal sources.\n\n### 🔸 Take a quick look\n* 🥇 Read the Data into a panda dataframe \n* 🥇 **.head()** check for the columns and values represented\n* 🥇 **.info()** check for non-null and datatypes presented\n* 🥇 **.value_counts()** check for each category columns occurances of value\n* 🥇 **.describe()** check for mean, min, max, std values of numerical columns\n\n### 🔸 Visualize the data\n* 🥇 **.hist()** to visualize each columns. \n* 🥇 check for outliers.\n* 🥇 check for scale and representations.\n* 🥇 check for the tail heavy charts and note them for further actions.\n\n### 🔸 Create test set\n* 🥇 using sklearn.modelSelection we can split the train and test data (80 - 20% usually but based on the size of the data)\n* 🥇 Take care of stratified shuffle split if the represntation of some columns are not distributed properly, This call can be taken during thr previous Visualize data step\n\n### 🔸 Check for correlation \n* 🥇 **.Corr()** method on df and check for corelation between numeric columns. ( The values range from -1 to 1. 0 means no correlation)\n\n### 🔸 Experiment Attribution Combinations\n* 🥇 To get lesser columns and much meaningfull columns do feature combination.\n* 🥇 Create derived columns such a way that the correlation to target is best fit.","metadata":{}},{"cell_type":"markdown","source":"# ⛳️ 3. Prepare the Data for ML Algo\n\n### 🔸 Preliminary Steps\n* 🥇 Make sure you create a function for each operation, this will be helpful while creating the pipeline\n* 🥇 Create a copy of train set (I usually keep same naming convention 'train_set') \n* 🥇 Seperate out labels and target from train_set (train_set_label, train_set_target)\n\n### 🔸 Data Cleaning\n* 🥇 Handle missing data\n* * 🥈 Missing Numerical data. \n* * * 🥉 **.dropna()** to drop the missing data complete rows \n* * * 🥉 **.drop()** to drop the missing complete columns\n* * * 🥉 **.fillna()** to fill the data. (mean, median, std, constant are some of the option to pass)\n\n* * 🥉 *Use Sklearn Simpleimputer to do the above, this is much better way to do* (mean, median, std, constant are some of the option to pass)\n* * 🥈 Missing categorical data. \n* * * 🥉 Use Sklearn 1HotEncoder for converting categorical data into numerical data (This will work if the value counts are limited, otherwise we will endup with lot of columns). \n* * * 🥉 Use ordinalEncoder to convert categorical data into uniq ID (numbers)\n\n\n### 🔸 Feature scaling\n* 🥇 From above steps we have all the columns in place we need to scale all the columns (normalizing, standardaization)\n* 🥇 You can make use of SKLearns.preprocessing MinMaxScaler or StdScaler for the same\n* 🥇 If we have any tail heavy columns, while scaling we can make use of log method to scale\n* 🥇 If we have multiple peaks we can go for RBF for scaling.\n\n\n### 🔸 Setup Transformation Pipelines\n* 🥇 use SKLearn.pipeline to create pipelines for above steps\n* 🥇 Create ColumnTransofrmers which runs all the above pipelines ","metadata":{}},{"cell_type":"markdown","source":"# ⛳️ 4. Select and Train Model\n\n### 🔸 Select Model\n* 🥇 Try to use available models to gauge every models performance (LinearRegresor, Decision Tree, Random Forest)\n* 🥇 Check each models performance with the our Performance measure we decided previously (train_set_label, train_set_target and predict the test_set without target and then compare prediction with target) \n\n### 🔸 Evaluate Model\n* 🥇 If it performed poor in train_set itself then we are underfitting. We need to go for a powerfull model than the one we use\n* 🥇 If it performed best in train_set (with RMSE ~0) and performed poor with test set then there are chances of overfitting. \n* 🥇 For overfitting we can evaluate the model and try Crossvalidations\n* 🥇 Always its better to have 2-5 models shortlisted at the end of this step. which can be finetuned further.","metadata":{}},{"cell_type":"markdown","source":"# ⛳️ 5. Fine-Tune the Model\n* 🥇 We can use grid search CV, Randomized Search CV\n* 🥇 We can use Ensembling to take all models and check the errors\n* 🥇 Find best estimators (hyperparam tuning)","metadata":{}},{"cell_type":"markdown","source":"# ⛳️ 6. Evaluate the Model with Test set","metadata":{}},{"cell_type":"markdown","source":"# ⛳️ 7. Launch the model","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}