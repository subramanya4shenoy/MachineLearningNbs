{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/subramanyashenoy/blue-print-for-regression?scriptVersionId=142244928\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"29afbd29","metadata":{"papermill":{"duration":0.003759,"end_time":"2023-09-07T14:15:31.465194","exception":false,"start_time":"2023-09-07T14:15:31.461435","status":"completed"},"tags":[]},"source":["# ⛳️ 1. Look at the Big picture \n","🟢 Read, Understand the Problem\n","\n","### 🔸 Frame the problem\n","* 🥇 Note what we want to acheive.\n","* 🥇 Form a hypothesis and decide the end goal and identify the format to acheive.\n","* 🥇 Decide on :\n","* * 🥈 Learning? : Supervised / Unsupervised / Reinfored\n","* * 🥈 Problem type? : Classification/Regression\n","* * 🥈 Univariate/BiVariate ?\n","* * 🥈 Single target or Multi target ?\n","\n","### 🔸 Select Performance measure\n","* 🥇  RMSE\n","* 🥇  MAE (best fit if there are many outliers)\n","\n","### 🔸 Check for Assumptions\n","* 🥇 Check for general information about the topic. Get the assumptions and features which needs more conciderations. etc.."]},{"cell_type":"markdown","id":"ad8d886e","metadata":{"papermill":{"duration":0.002761,"end_time":"2023-09-07T14:15:31.471142","exception":false,"start_time":"2023-09-07T14:15:31.468381","status":"completed"},"tags":[]},"source":["# ⛳️ 2. Get the Data\n","🟢 Make sure you collect the data from trusted sources and legal sources.\n","\n","### 🔸 Take a quick look\n","* 🥇 Read the Data into a panda dataframe \n","* 🥇 **.head()** check for the columns and values represented\n","* 🥇 **.info()** check for non-null and datatypes presented\n","* 🥇 **.value_counts()** check for each category columns occurances of value\n","* 🥇 **.describe()** check for mean, min, max, std values of numerical columns\n","\n","### 🔸 Visualize the data\n","* 🥇 **.hist()** to visualize each columns. \n","* 🥇 check for outliers.\n","* 🥇 check for scale and representations.\n","* 🥇 check for the tail heavy charts and note them for further actions.\n","\n","### 🔸 Create test set\n","* 🥇 using sklearn.modelSelection we can split the train and test data (80 - 20% usually but based on the size of the data)\n","* 🥇 Take care of stratified shuffle split if the represntation of some columns are not distributed properly, This call can be taken during thr previous Visualize data step\n","\n","### 🔸 Check for correlation \n","* 🥇 **.Corr()** method on df and check for corelation between numeric columns. ( The values range from -1 to 1. 0 means no correlation)\n","\n","### 🔸 Experiment Attribution Combinations\n","* 🥇 To get lesser columns and much meaningfull columns do feature combination.\n","* 🥇 Create derived columns such a way that the correlation to target is best fit."]},{"cell_type":"markdown","id":"457190b8","metadata":{"papermill":{"duration":0.002747,"end_time":"2023-09-07T14:15:31.476764","exception":false,"start_time":"2023-09-07T14:15:31.474017","status":"completed"},"tags":[]},"source":["# ⛳️ 3. Prepare the Data for ML Algo\n","\n","### 🔸 Preliminary Steps\n","* 🥇 Make sure you create a function for each operation, this will be helpful while creating the pipeline\n","* 🥇 Create a copy of train set (I usually keep same naming convention 'train_set') \n","* 🥇 Seperate out labels and target from train_set (train_set_label, train_set_target)\n","\n","### 🔸 Data Cleaning\n","* 🥇 Handle missing data\n","* * 🥈 Missing Numerical data. \n","* * * 🥉 **.dropna()** to drop the missing data complete rows \n","* * * 🥉 **.drop()** to drop the missing complete columns\n","* * * 🥉 **.fillna()** to fill the data. (mean, median, std, constant are some of the option to pass)\n","\n","* * 🥉 *Use Sklearn Simpleimputer to do the above, this is much better way to do* (mean, median, std, constant are some of the option to pass)\n","* * 🥈 Missing categorical data. \n","* * * 🥉 Use Sklearn 1HotEncoder for converting categorical data into numerical data (This will work if the value counts are limited, otherwise we will endup with lot of columns). \n","* * * 🥉 Use ordinalEncoder to convert categorical data into uniq ID (numbers)\n","\n","\n","### 🔸 Feature scaling\n","* 🥇 From above steps we have all the columns in place we need to scale all the columns (normalizing, standardaization)\n","* 🥇 You can make use of SKLearns.preprocessing MinMaxScaler or StdScaler for the same\n","* 🥇 If we have any tail heavy columns, while scaling we can make use of log method to scale\n","* 🥇 If we have multiple peaks we can go for RBF for scaling.\n","\n","\n","### 🔸 Setup Transformation Pipelines\n","* 🥇 use SKLearn.pipeline to create pipelines for above steps\n","* 🥇 Create ColumnTransofrmers which runs all the above pipelines "]},{"cell_type":"markdown","id":"29246f22","metadata":{"papermill":{"duration":0.003489,"end_time":"2023-09-07T14:15:31.483083","exception":false,"start_time":"2023-09-07T14:15:31.479594","status":"completed"},"tags":[]},"source":["# ⛳️ 4. Select and Train Model\n","\n","### 🔸 Select Model\n","* 🥇 Try to use available models to gauge every models performance (LinearRegresor, Decision Tree, Random Forest)\n","* 🥇 Check each models performance with the our Performance measure we decided previously (train_set_label, train_set_target and predict the test_set without target and then compare prediction with target) \n","\n","### 🔸 Evaluate Model\n","* 🥇 If it performed poor in train_set itself then we are underfitting. We need to go for a powerfull model than the one we use\n","* 🥇 If it performed best in train_set (with RMSE ~0) and performed poor with test set then there are chances of overfitting. \n","* 🥇 For overfitting we can evaluate the model and try Crossvalidations\n","* 🥇 Always its better to have 2-5 models shortlisted at the end of this step. which can be finetuned further."]},{"cell_type":"markdown","id":"1a3ca2bf","metadata":{"papermill":{"duration":0.003125,"end_time":"2023-09-07T14:15:31.490046","exception":false,"start_time":"2023-09-07T14:15:31.486921","status":"completed"},"tags":[]},"source":["# ⛳️ 5. Fine-Tune the Model\n","* 🥇 We can use grid search CV, Randomized Search CV\n","* 🥇 We can use Ensembling to take all models and check the errors\n","* 🥇 Find best estimators (hyperparam tuning)\n","\n","\n"]},{"cell_type":"markdown","id":"18574786","metadata":{"papermill":{"duration":0.002963,"end_time":"2023-09-07T14:15:31.496258","exception":false,"start_time":"2023-09-07T14:15:31.493295","status":"completed"},"tags":[]},"source":["# ⛳️ 6. Evaluate the Model with Test set"]},{"cell_type":"markdown","id":"d7459c18","metadata":{"papermill":{"duration":0.002896,"end_time":"2023-09-07T14:15:31.502299","exception":false,"start_time":"2023-09-07T14:15:31.499403","status":"completed"},"tags":[]},"source":["# ⛳️ 7. Launch the model"]},{"cell_type":"code","execution_count":null,"id":"5684c982","metadata":{"papermill":{"duration":0.002842,"end_time":"2023-09-07T14:15:31.508475","exception":false,"start_time":"2023-09-07T14:15:31.505633","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":3.386715,"end_time":"2023-09-07T14:15:31.830632","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-09-07T14:15:28.443917","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}