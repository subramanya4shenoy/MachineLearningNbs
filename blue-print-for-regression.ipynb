{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/subramanyashenoy/blue-print-for-regression?scriptVersionId=142244928\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"29afbd29","metadata":{"papermill":{"duration":0.003759,"end_time":"2023-09-07T14:15:31.465194","exception":false,"start_time":"2023-09-07T14:15:31.461435","status":"completed"},"tags":[]},"source":["# â›³ï¸ 1. Look at the Big picture \n","ğŸŸ¢ Read, Understand the Problem\n","\n","### ğŸ”¸ Frame the problem\n","* ğŸ¥‡ Note what we want to acheive.\n","* ğŸ¥‡ Form a hypothesis and decide the end goal and identify the format to acheive.\n","* ğŸ¥‡ Decide on :\n","* * ğŸ¥ˆ Learning? : Supervised / Unsupervised / Reinfored\n","* * ğŸ¥ˆ Problem type? : Classification/Regression\n","* * ğŸ¥ˆ Univariate/BiVariate ?\n","* * ğŸ¥ˆ Single target or Multi target ?\n","\n","### ğŸ”¸ Select Performance measure\n","* ğŸ¥‡  RMSE\n","* ğŸ¥‡  MAE (best fit if there are many outliers)\n","\n","### ğŸ”¸ Check for Assumptions\n","* ğŸ¥‡ Check for general information about the topic. Get the assumptions and features which needs more conciderations. etc.."]},{"cell_type":"markdown","id":"ad8d886e","metadata":{"papermill":{"duration":0.002761,"end_time":"2023-09-07T14:15:31.471142","exception":false,"start_time":"2023-09-07T14:15:31.468381","status":"completed"},"tags":[]},"source":["# â›³ï¸ 2. Get the Data\n","ğŸŸ¢ Make sure you collect the data from trusted sources and legal sources.\n","\n","### ğŸ”¸ Take a quick look\n","* ğŸ¥‡ Read the Data into a panda dataframe \n","* ğŸ¥‡ **.head()** check for the columns and values represented\n","* ğŸ¥‡ **.info()** check for non-null and datatypes presented\n","* ğŸ¥‡ **.value_counts()** check for each category columns occurances of value\n","* ğŸ¥‡ **.describe()** check for mean, min, max, std values of numerical columns\n","\n","### ğŸ”¸ Visualize the data\n","* ğŸ¥‡ **.hist()** to visualize each columns. \n","* ğŸ¥‡ check for outliers.\n","* ğŸ¥‡ check for scale and representations.\n","* ğŸ¥‡ check for the tail heavy charts and note them for further actions.\n","\n","### ğŸ”¸ Create test set\n","* ğŸ¥‡ using sklearn.modelSelection we can split the train and test data (80 - 20% usually but based on the size of the data)\n","* ğŸ¥‡ Take care of stratified shuffle split if the represntation of some columns are not distributed properly, This call can be taken during thr previous Visualize data step\n","\n","### ğŸ”¸ Check for correlation \n","* ğŸ¥‡ **.Corr()** method on df and check for corelation between numeric columns. ( The values range from -1 to 1. 0 means no correlation)\n","\n","### ğŸ”¸ Experiment Attribution Combinations\n","* ğŸ¥‡ To get lesser columns and much meaningfull columns do feature combination.\n","* ğŸ¥‡ Create derived columns such a way that the correlation to target is best fit."]},{"cell_type":"markdown","id":"457190b8","metadata":{"papermill":{"duration":0.002747,"end_time":"2023-09-07T14:15:31.476764","exception":false,"start_time":"2023-09-07T14:15:31.474017","status":"completed"},"tags":[]},"source":["# â›³ï¸ 3. Prepare the Data for ML Algo\n","\n","### ğŸ”¸ Preliminary Steps\n","* ğŸ¥‡ Make sure you create a function for each operation, this will be helpful while creating the pipeline\n","* ğŸ¥‡ Create a copy of train set (I usually keep same naming convention 'train_set') \n","* ğŸ¥‡ Seperate out labels and target from train_set (train_set_label, train_set_target)\n","\n","### ğŸ”¸ Data Cleaning\n","* ğŸ¥‡ Handle missing data\n","* * ğŸ¥ˆ Missing Numerical data. \n","* * * ğŸ¥‰ **.dropna()** to drop the missing data complete rows \n","* * * ğŸ¥‰ **.drop()** to drop the missing complete columns\n","* * * ğŸ¥‰ **.fillna()** to fill the data. (mean, median, std, constant are some of the option to pass)\n","\n","* * ğŸ¥‰ *Use Sklearn Simpleimputer to do the above, this is much better way to do* (mean, median, std, constant are some of the option to pass)\n","* * ğŸ¥ˆ Missing categorical data. \n","* * * ğŸ¥‰ Use Sklearn 1HotEncoder for converting categorical data into numerical data (This will work if the value counts are limited, otherwise we will endup with lot of columns). \n","* * * ğŸ¥‰ Use ordinalEncoder to convert categorical data into uniq ID (numbers)\n","\n","\n","### ğŸ”¸ Feature scaling\n","* ğŸ¥‡ From above steps we have all the columns in place we need to scale all the columns (normalizing, standardaization)\n","* ğŸ¥‡ You can make use of SKLearns.preprocessing MinMaxScaler or StdScaler for the same\n","* ğŸ¥‡ If we have any tail heavy columns, while scaling we can make use of log method to scale\n","* ğŸ¥‡ If we have multiple peaks we can go for RBF for scaling.\n","\n","\n","### ğŸ”¸ Setup Transformation Pipelines\n","* ğŸ¥‡ use SKLearn.pipeline to create pipelines for above steps\n","* ğŸ¥‡ Create ColumnTransofrmers which runs all the above pipelines "]},{"cell_type":"markdown","id":"29246f22","metadata":{"papermill":{"duration":0.003489,"end_time":"2023-09-07T14:15:31.483083","exception":false,"start_time":"2023-09-07T14:15:31.479594","status":"completed"},"tags":[]},"source":["# â›³ï¸ 4. Select and Train Model\n","\n","### ğŸ”¸ Select Model\n","* ğŸ¥‡ Try to use available models to gauge every models performance (LinearRegresor, Decision Tree, Random Forest)\n","* ğŸ¥‡ Check each models performance with the our Performance measure we decided previously (train_set_label, train_set_target and predict the test_set without target and then compare prediction with target) \n","\n","### ğŸ”¸ Evaluate Model\n","* ğŸ¥‡ If it performed poor in train_set itself then we are underfitting. We need to go for a powerfull model than the one we use\n","* ğŸ¥‡ If it performed best in train_set (with RMSE ~0) and performed poor with test set then there are chances of overfitting. \n","* ğŸ¥‡ For overfitting we can evaluate the model and try Crossvalidations\n","* ğŸ¥‡ Always its better to have 2-5 models shortlisted at the end of this step. which can be finetuned further."]},{"cell_type":"markdown","id":"1a3ca2bf","metadata":{"papermill":{"duration":0.003125,"end_time":"2023-09-07T14:15:31.490046","exception":false,"start_time":"2023-09-07T14:15:31.486921","status":"completed"},"tags":[]},"source":["# â›³ï¸ 5. Fine-Tune the Model\n","* ğŸ¥‡ We can use grid search CV, Randomized Search CV\n","* ğŸ¥‡ We can use Ensembling to take all models and check the errors\n","* ğŸ¥‡ Find best estimators (hyperparam tuning)\n","\n","\n"]},{"cell_type":"markdown","id":"18574786","metadata":{"papermill":{"duration":0.002963,"end_time":"2023-09-07T14:15:31.496258","exception":false,"start_time":"2023-09-07T14:15:31.493295","status":"completed"},"tags":[]},"source":["# â›³ï¸ 6. Evaluate the Model with Test set"]},{"cell_type":"markdown","id":"d7459c18","metadata":{"papermill":{"duration":0.002896,"end_time":"2023-09-07T14:15:31.502299","exception":false,"start_time":"2023-09-07T14:15:31.499403","status":"completed"},"tags":[]},"source":["# â›³ï¸ 7. Launch the model"]},{"cell_type":"code","execution_count":null,"id":"5684c982","metadata":{"papermill":{"duration":0.002842,"end_time":"2023-09-07T14:15:31.508475","exception":false,"start_time":"2023-09-07T14:15:31.505633","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":3.386715,"end_time":"2023-09-07T14:15:31.830632","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-09-07T14:15:28.443917","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}