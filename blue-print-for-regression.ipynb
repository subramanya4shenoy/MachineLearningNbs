{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/subramanyashenoy/blue-print-for-regression?scriptVersionId=142136282\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","id":"ad20a4d0","metadata":{"papermill":{"duration":0.005044,"end_time":"2023-09-06T15:51:17.057538","exception":false,"start_time":"2023-09-06T15:51:17.052494","status":"completed"},"tags":[]},"source":["# ⛳️ 1. Look at the Big picture \n","🟢 Read, Understand the Problem\n","\n","### 🔸 Frame the problem\n","* 🥇 Note what we want to acheive.\n","* 🥇 Form a hypothesis and decide the end goal and identify the format to acheive.\n","* 🥇 Decide on :\n","* * 🥈 Suppervised/Unsupervised/Reinforced/Classification/Regression ?\n","* * 🥈 Univariate/BiVariate ?\n","* * 🥈 Single target or Multi target ?\n","\n","### 🔸 Select Performance measure\n","* 🥇  RMSE\n","* 🥇  MAE (best fit if there are many outliers)\n","\n","### 🔸 Check for Assumptions\n","* 🥇 Check for general information about the topic. Get the assumptions and features which needs more conciderations. etc.."]},{"cell_type":"markdown","id":"5d0c1c0f","metadata":{"papermill":{"duration":0.004386,"end_time":"2023-09-06T15:51:17.066659","exception":false,"start_time":"2023-09-06T15:51:17.062273","status":"completed"},"tags":[]},"source":["# ⛳️ 2. Get the Data\n","🟢 Make sure you collect the data from trusted sources and legal sources.\n","\n","### 🔸 Take a quick look\n","* 🥇 Read the Data into a panda dataframe \n","* 🥇 **.head()** check for the columns and values represented\n","* 🥇 **.info()** check for non-null and datatypes presented\n","* 🥇 **.value_counts()** check for each category columns occurances of value\n","* 🥇 **.describe()** check for mean, min, max, std values of numerical columns\n","\n","### 🔸 Visualize the data\n","* 🥇 **.hist()** to visualize each columns. \n","* 🥇 check for outliers.\n","* 🥇 check for scale and representations.\n","* 🥇 check for the tail heavy charts and note them for further actions.\n","\n","### 🔸 Create test set\n","* 🥇 using sklearn.modelSelection we can split the train and test data (80 - 20% usually but based on the size of the data)\n","* 🥇 Take care of stratified shuffle split if the represntation of some columns are not distributed properly, This call can be taken during thr previous Visualize data step\n","\n","### 🔸 Check for correlation \n","* 🥇 **.Corr()** method on df and check for corelation between numeric columns. ( The values range from -1 to 1. 0 means no correlation)\n","\n","### 🔸 Experiment Attribution Combinations\n","* 🥇 To get lesser columns and much meaningfull columns do feature combination.\n","* 🥇 Create derived columns such a way that the correlation to target is best fit."]},{"cell_type":"markdown","id":"9ef3394d","metadata":{"papermill":{"duration":0.004171,"end_time":"2023-09-06T15:51:17.075283","exception":false,"start_time":"2023-09-06T15:51:17.071112","status":"completed"},"tags":[]},"source":["# ⛳️ 3. Prepare the Data for ML Algo\n","\n","### 🔸 Preliminary Steps\n","* 🥇 Make sure you create a function for each operation, this will be helpful while creating the pipeline\n","* 🥇 Create a copy of train set (I usually keep same naming convention 'train_set') \n","* 🥇 Seperate out labels and target from train_set (train_set_label, train_set_target)\n","\n","### 🔸 Data Cleaning\n","* 🥇 Handle missing data\n","* * 🥈 Missing Numerical data. \n","* * * 🥉 **.dropna()** to drop the missing data complete rows \n","* * * 🥉 **.drop()** to drop the missing complete columns\n","* * * 🥉 **.fillna()** to fill the data. (mean, median, std, constant are some of the option to pass)\n","\n","* * 🥉 *Use Sklearn Simpleimputer to do the above, this is much better way to do* (mean, median, std, constant are some of the option to pass)\n","* * 🥈 Missing categorical data. \n","* * * 🥉 Use Sklearn 1HotEncoder for converting categorical data into numerical data (This will work if the value counts are limited, otherwise we will endup with lot of columns). \n","* * * 🥉 Use ordinalEncoder to convert categorical data into uniq ID (numbers)\n","\n","\n","### 🔸 Feature scaling\n","* 🥇 From above steps we have all the columns in place we need to scale all the columns (normalizing, standardaization)\n","* 🥇 You can make use of SKLearns.preprocessing MinMaxScaler or StdScaler for the same\n","* 🥇 If we have any tail heavy columns, while scaling we can make use of log method to scale\n","* 🥇 If we have multiple peaks we can go for RBF for scaling.\n","\n","\n","### 🔸 Setup Transformation Pipelines\n","* 🥇 use SKLearn.pipeline to create pipelines for above steps\n","* 🥇 Create ColumnTransofrmers which runs all the above pipelines "]},{"cell_type":"markdown","id":"1618910d","metadata":{"papermill":{"duration":0.004137,"end_time":"2023-09-06T15:51:17.08381","exception":false,"start_time":"2023-09-06T15:51:17.079673","status":"completed"},"tags":[]},"source":["# ⛳️ 4. Select and Train Model\n","\n","### 🔸 Select Model\n","* 🥇 Try to use available models to gauge every models performance (LinearRegresor, Decision Tree, Random Forest)\n","* 🥇 Check each models performance with the our Performance measure we decided previously (train_set_label, train_set_target and predict the test_set without target and then compare prediction with target) \n","\n","### 🔸 Evaluate Model\n","* 🥇 If it performed poor in train_set itself then we are underfitting. We need to go for a powerfull model than the one we use\n","* 🥇 If it performed best in train_set (with RMSE ~0) and performed poor with test set then there are chances of overfitting. \n","* 🥇 For overfitting we can evaluate the model and try Crossvalidations\n","* 🥇 Always its better to have 2-5 models shortlisted at the end of this step. which can be finetuned further."]},{"cell_type":"markdown","id":"7fdec995","metadata":{"papermill":{"duration":0.004204,"end_time":"2023-09-06T15:51:17.092523","exception":false,"start_time":"2023-09-06T15:51:17.088319","status":"completed"},"tags":[]},"source":["# ⛳️ 5. Fine-Tune the Model\n","* 🥇 We can use grid search CV, Randomized Search CV\n","* 🥇 We can use Ensembling to take all models and check the errors\n","* 🥇 Find best estimators (hyperparam tuning)\n","\n","\n"]},{"cell_type":"markdown","id":"83521e48","metadata":{"papermill":{"duration":0.004208,"end_time":"2023-09-06T15:51:17.101105","exception":false,"start_time":"2023-09-06T15:51:17.096897","status":"completed"},"tags":[]},"source":["# ⛳️ 6. Evaluate the Model with Test set"]},{"cell_type":"markdown","id":"14eb8ee8","metadata":{"papermill":{"duration":0.004172,"end_time":"2023-09-06T15:51:17.109672","exception":false,"start_time":"2023-09-06T15:51:17.1055","status":"completed"},"tags":[]},"source":["# ⛳️ 7. Launch the model"]},{"cell_type":"code","execution_count":null,"id":"4fc375b8","metadata":{"papermill":{"duration":0.004188,"end_time":"2023-09-06T15:51:17.118439","exception":false,"start_time":"2023-09-06T15:51:17.114251","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"papermill":{"default_parameters":{},"duration":4.063317,"end_time":"2023-09-06T15:51:17.543102","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-09-06T15:51:13.479785","version":"2.4.0"}},"nbformat":4,"nbformat_minor":5}